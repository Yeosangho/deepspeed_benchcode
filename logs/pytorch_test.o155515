==========================================
SLURM_JOB_ID = 155515
SLURM_NODELIST = gpu[19-20]
==========================================
NODELIST=gpu[19-20]
MASTER_ADDR=gpu19
      Id  ...                                               Text
0      1  ...  I have bought several of the Vitality canned d...
1      2  ...  Product arrived labeled as Jumbo Salted Peanut...
2      3  ...  This is a confection that has been around a fe...
3      4  ...  If you are looking for the secret ingredient i...
4      5  ...  Great taffy at a great price.  There was a wid...
..   ...  ...                                                ...
595  596  ...  This is the second purchase of Kettle Potato C...
596  597  ...  How to achieve potato chip nirvana? It's simpl...
597  598  ...  I found these more than 10 years ago in a heal...
598  599  ...  But you will enjoy ever step. I gained 5 lbs w...
599  600  ...  These are the best spicy chips I have ever had...

[600 rows x 10 columns]
600
[2022-12-11 19:43:15,615] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.7.0, git-hash=unknown, git-branch=unknown
[2022-12-11 19:43:16,453] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2022-12-11 19:43:16,454] [INFO] [logging.py:68:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer
[2022-12-11 19:43:16,454] [INFO] [logging.py:68:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2022-12-11 19:43:16,458] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Basic Optimizer = {basic_optimizer.__class__.__name__}
[2022-12-11 19:43:16,458] [INFO] [utils.py:52:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>
[2022-12-11 19:43:16,458] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 ZeRO stage 2 optimizer
[2022-12-11 19:43:16,458] [INFO] [stage_1_and_2.py:134:__init__] Reduce bucket size 0
[2022-12-11 19:43:16,458] [INFO] [stage_1_and_2.py:135:__init__] Allgather bucket size 0
[2022-12-11 19:43:16,458] [INFO] [stage_1_and_2.py:136:__init__] CPU Offload: False
[2022-12-11 19:43:16,458] [INFO] [stage_1_and_2.py:137:__init__] Round robin gradient partitioning: False
      Id  ...                                               Text
0      1  ...  I have bought several of the Vitality canned d...
1      2  ...  Product arrived labeled as Jumbo Salted Peanut...
2      3  ...  This is a confection that has been around a fe...
3      4  ...  If you are looking for the secret ingredient i...
4      5  ...  Great taffy at a great price.  There was a wid...
..   ...  ...                                                ...
595  596  ...  This is the second purchase of Kettle Potato C...
596  597  ...  How to achieve potato chip nirvana? It's simpl...
597  598  ...  I found these more than 10 years ago in a heal...
598  599  ...  But you will enjoy ever step. I gained 5 lbs w...
599  600  ...  These are the best spicy chips I have ever had...

[600 rows x 10 columns]
600
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Emitting ninja build file /home01/hpc72a03/.cache/torch_extensions/py38_cu113/utils/build.ninja...
Building extension module utils...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module utils...
Time to load utils op: 0.3429086208343506 seconds
Rank: 7 partition count [8] and sizes[(15555168, False)] 
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0009770393371582031 seconds
      Id  ...                                               Text
0      1  ...  I have bought several of the Vitality canned d...
1      2  ...  Product arrived labeled as Jumbo Salted Peanut...
2      3  ...  This is a confection that has been around a fe...
3      4  ...  If you are looking for the secret ingredient i...
4      5  ...  Great taffy at a great price.  There was a wid...
..   ...  ...                                                ...
595  596  ...  This is the second purchase of Kettle Potato C...
596  597  ...  How to achieve potato chip nirvana? It's simpl...
597  598  ...  I found these more than 10 years ago in a heal...
598  599  ...  But you will enjoy ever step. I gained 5 lbs w...
599  600  ...  These are the best spicy chips I have ever had...

[600 rows x 10 columns]
600
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.40389227867126465 seconds
      Id  ...                                               Text
0      1  ...  I have bought several of the Vitality canned d...
1      2  ...  Product arrived labeled as Jumbo Salted Peanut...
2      3  ...  This is a confection that has been around a fe...
3      4  ...  If you are looking for the secret ingredient i...
4      5  ...  Great taffy at a great price.  There was a wid...
..   ...  ...                                                ...
595  596  ...  This is the second purchase of Kettle Potato C...
596  597  ...  How to achieve potato chip nirvana? It's simpl...
597  598  ...  I found these more than 10 years ago in a heal...
598  599  ...  But you will enjoy ever step. I gained 5 lbs w...
599  600  ...  These are the best spicy chips I have ever had...

[600 rows x 10 columns]
600
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.3042628765106201 seconds
Rank: 6 partition count [8] and sizes[(15555168, False)] 
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0009279251098632812 seconds
Rank: 1 partition count [8] and sizes[(15555168, False)] 
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0006899833679199219 seconds
      Id  ...                                               Text
0      1  ...  I have bought several of the Vitality canned d...
1      2  ...  Product arrived labeled as Jumbo Salted Peanut...
2      3  ...  This is a confection that has been around a fe...
3      4  ...  If you are looking for the secret ingredient i...
4      5  ...  Great taffy at a great price.  There was a wid...
..   ...  ...                                                ...
595  596  ...  This is the second purchase of Kettle Potato C...
596  597  ...  How to achieve potato chip nirvana? It's simpl...
597  598  ...  I found these more than 10 years ago in a heal...
598  599  ...  But you will enjoy ever step. I gained 5 lbs w...
599  600  ...  These are the best spicy chips I have ever had...

[600 rows x 10 columns]
600
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.30356740951538086 seconds
      Id  ...                                               Text
0      1  ...  I have bought several of the Vitality canned d...
1      2  ...  Product arrived labeled as Jumbo Salted Peanut...
2      3  ...  This is a confection that has been around a fe...
3      4  ...  If you are looking for the secret ingredient i...
4      5  ...  Great taffy at a great price.  There was a wid...
..   ...  ...                                                ...
595  596  ...  This is the second purchase of Kettle Potato C...
596  597  ...  How to achieve potato chip nirvana? It's simpl...
597  598  ...  I found these more than 10 years ago in a heal...
598  599  ...  But you will enjoy ever step. I gained 5 lbs w...
599  600  ...  These are the best spicy chips I have ever had...

[600 rows x 10 columns]
600
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4058268070220947 seconds
Rank: 5 partition count [8] and sizes[(15555168, False)] 
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0008747577667236328 seconds
Rank: 2 partition count [8] and sizes[(15555168, False)] 
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.001378774642944336 seconds
      Id  ...                                               Text
0      1  ...  I have bought several of the Vitality canned d...
1      2  ...  Product arrived labeled as Jumbo Salted Peanut...
2      3  ...  This is a confection that has been around a fe...
3      4  ...  If you are looking for the secret ingredient i...
4      5  ...  Great taffy at a great price.  There was a wid...
..   ...  ...                                                ...
595  596  ...  This is the second purchase of Kettle Potato C...
596  597  ...  How to achieve potato chip nirvana? It's simpl...
597  598  ...  I found these more than 10 years ago in a heal...
598  599  ...  But you will enjoy ever step. I gained 5 lbs w...
599  600  ...  These are the best spicy chips I have ever had...

[600 rows x 10 columns]
600
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.40427112579345703 seconds
      Id  ...                                               Text
0      1  ...  I have bought several of the Vitality canned d...
1      2  ...  Product arrived labeled as Jumbo Salted Peanut...
2      3  ...  This is a confection that has been around a fe...
3      4  ...  If you are looking for the secret ingredient i...
4      5  ...  Great taffy at a great price.  There was a wid...
..   ...  ...                                                ...
595  596  ...  This is the second purchase of Kettle Potato C...
596  597  ...  How to achieve potato chip nirvana? It's simpl...
597  598  ...  I found these more than 10 years ago in a heal...
598  599  ...  But you will enjoy ever step. I gained 5 lbs w...
599  600  ...  These are the best spicy chips I have ever had...

[600 rows x 10 columns]
600
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.40519046783447266 seconds
Rank: 4 partition count [8] and sizes[(15555168, False)] 
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0009322166442871094 seconds
Rank: 3 partition count [8] and sizes[(15555168, False)] 
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0012035369873046875 seconds
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
Loading extension module utils...
Time to load utils op: 0.4042015075683594 seconds
Rank: 0 partition count [8] and sizes[(15555168, False)] 
[2022-12-11 19:43:18,118] [INFO] [utils.py:827:see_memory_usage] Before initializing optimizer states
[2022-12-11 19:43:18,119] [INFO] [utils.py:828:see_memory_usage] MA 0.53 GB         Max_MA 0.53 GB         CA 0.99 GB         Max_CA 1 GB 
[2022-12-11 19:43:18,120] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 22.92 GB, percent = 6.1%
[2022-12-11 19:43:18,178] [INFO] [utils.py:827:see_memory_usage] After initializing optimizer states
[2022-12-11 19:43:18,179] [INFO] [utils.py:828:see_memory_usage] MA 0.65 GB         Max_MA 0.83 GB         CA 1.23 GB         Max_CA 1 GB 
[2022-12-11 19:43:18,180] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 23.0 GB, percent = 6.1%
[2022-12-11 19:43:18,180] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized
[2022-12-11 19:43:18,235] [INFO] [utils.py:827:see_memory_usage] After initializing ZeRO optimizer
[2022-12-11 19:43:18,235] [INFO] [utils.py:828:see_memory_usage] MA 0.65 GB         Max_MA 0.65 GB         CA 1.23 GB         Max_CA 1 GB 
[2022-12-11 19:43:18,236] [INFO] [utils.py:836:see_memory_usage] CPU Virtual Memory:  used = 23.07 GB, percent = 6.1%
[2022-12-11 19:43:18,236] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam
[2022-12-11 19:43:18,236] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2022-12-11 19:43:18,236] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x2b0aee80fee0>
[2022-12-11 19:43:18,236] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[0.1], mom=[(0.9, 0.999)]
[2022-12-11 19:43:18,237] [INFO] [config.py:975:print] DeepSpeedEngine configuration:
[2022-12-11 19:43:18,237] [INFO] [config.py:979:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2022-12-11 19:43:18,237] [INFO] [config.py:979:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2022-12-11 19:43:18,237] [INFO] [config.py:979:print]   amp_enabled .................. False
[2022-12-11 19:43:18,237] [INFO] [config.py:979:print]   amp_params ................... False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": null, 
    "exps_dir": null, 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   bfloat16_enabled ............. False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   checkpoint_tag_validation_enabled  True
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   checkpoint_tag_validation_fail  False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x2b0afdc50130>
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   communication_data_type ...... None
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   curriculum_enabled ........... False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   curriculum_params ............ False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   dataloader_drop_last ......... False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   disable_allgather ............ False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   dump_state ................... False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   dynamic_loss_scale_args ...... None
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   eigenvalue_enabled ........... False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   eigenvalue_gas_boundary_resolution  1
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   eigenvalue_layer_num ......... 0
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   eigenvalue_max_iter .......... 100
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   eigenvalue_stability ......... 1e-06
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   eigenvalue_tol ............... 0.01
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   eigenvalue_verbose ........... False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   elasticity_enabled ........... False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   flops_profiler_config ........ {
    "enabled": false, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   fp16_auto_cast ............... None
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   fp16_enabled ................. False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   fp16_master_weights_and_gradients  False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   global_rank .................. 0
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   gradient_accumulation_steps .. 1
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   gradient_clipping ............ 1.0
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   gradient_predivide_factor .... 1.0
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   initial_dynamic_scale ........ 4294967296
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   load_universal_checkpoint .... False
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   loss_scale ................... 0
[2022-12-11 19:43:18,238] [INFO] [config.py:979:print]   memory_breakdown ............. False
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   monitor_config ............... <deepspeed.monitor.config.DeepSpeedMonitorConfig object at 0x2b0afdc50100>
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   optimizer_legacy_fusion ...... False
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   optimizer_name ............... adam
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   pld_enabled .................. False
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   pld_params ................... False
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   prescale_gradients ........... False
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   scheduler_name ............... WarmupLR
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   sparse_attention ............. None
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   sparse_gradients_enabled ..... False
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   steps_per_print .............. 2000
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   train_batch_size ............. 16
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   train_micro_batch_size_per_gpu  2
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   wall_clock_breakdown ......... False
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   world_size ................... 8
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   zero_allow_untested_optimizer  False
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=0 allgather_partitions=True allgather_bucket_size=0 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=0 max_reuse_distance=0 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   zero_enabled ................. True
[2022-12-11 19:43:18,239] [INFO] [config.py:979:print]   zero_optimization_stage ...... 2
[2022-12-11 19:43:18,239] [INFO] [config.py:981:print]   json = {
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 2, 
    "steps_per_print": 2.000000e+03, 
    "optimizer": {
        "type": "Adam", 
        "params": {
            "lr": 0.001, 
            "betas": [0.8, 0.999], 
            "eps": 1e-08, 
            "weight_decay": 3e-07
        }
    }, 
    "scheduler": {
        "type": "WarmupLR", 
        "params": {
            "warmup_min_lr": 0, 
            "warmup_max_lr": 0.001, 
            "warmup_num_steps": 1000
        }
    }, 
    "gradient_clipping": 1.0, 
    "prescale_gradients": false, 
    "fp16": {
        "enabled": false
    }, 
    "wall_clock_breakdown": false, 
    "zero_optimization": {
        "stage": 2, 
        "stage3_max_reuse_distance": 0, 
        "stage3_max_live_parameters": 0, 
        "allgather_bucket_size": 0, 
        "reduce_bucket_size": 0, 
        "overlap_comm": true
    }, 
    "compression_training": {
        "weight_quantization": {
            "shared_parameters": {
            }, 
            "different_groups": {
            }
        }, 
        "activation_quantization": {
            "shared_parameters": {
            }, 
            "different_groups": {
            }
        }, 
        "sparse_pruning": {
            "shared_parameters": {
            }, 
            "different_groups": {
            }
        }, 
        "row_pruning": {
            "shared_parameters": {
            }, 
            "different_groups": {
            }
        }, 
        "head_pruning": {
            "shared_parameters": {
            }, 
            "different_groups": {
            }
        }, 
        "channel_pruning": {
            "shared_parameters": {
            }, 
            "different_groups": {
            }
        }
    }
}
Using /home01/hpc72a03/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...
No modifications detected for re-loaded extension module utils, skipping build step...
Loading extension module utils...
Time to load utils op: 0.0007250308990478516 seconds
after forward allocated memory size is .. 4454.0361328125
after forward allocated memory size is .. 4454.0361328125
after forward allocated memory size is .. 4454.0361328125
after forward allocated memory size is .. 4454.0361328125
after forward allocated memory size is .. 4454.0361328125
after forward allocated memory size is .. 4454.0361328125
after forward allocated memory size is .. 4454.0361328125
tensor(17.9607, device='cuda:0', grad_fn=<NllLossBackward0>)
after forward allocated memory size is .. 4454.0361328125
